# -*- coding: utf-8 -*-
"""
ç´ æäººæ•ˆæ—¥æ±‡æ€»_v2.3.1.py
ä¿®å¤ï¼šSheet3 åˆ—é€‰æ‹©ç”¨äº† setï¼Œæ”¹ä¸º listï¼Œå¹¶åšç¼ºåˆ—å…œåº•ã€‚
å…¶ä½™é€»è¾‘åŒ v2.3ã€‚
"""
import os, re, glob, sys, traceback
from datetime import datetime
import numpy as np
import pandas as pd

OUT_XLSX = "ç´ æäººæ•ˆæ—¥æ±‡æ€»_v2.3.1.xlsx"
LOG_FILE = "run_log.txt"

class TeeLogger:
    def __init__(self, logfile):
        self.terminal = sys.stdout
        self.log = open(logfile, "a", encoding="utf-8")
    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()
    def flush(self):
        self.terminal.flush()
        self.log.flush()

sys.stdout = TeeLogger(LOG_FILE)

def now_str():
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def info(msg):
    print(f"[{now_str()}] {msg}")

def read_excel_df(fp: str, sheet_name=0) -> pd.DataFrame:
    return pd.read_excel(fp, sheet_name=sheet_name, dtype=str)

def read_excel_any(fp: str) -> pd.DataFrame:
    raw = pd.read_excel(fp, sheet_name=None, dtype=str)
    if "Sheet1" in raw and isinstance(raw["Sheet1"], pd.DataFrame):
        df = raw["Sheet1"]
        if len(df.columns)>0:
            return df
    for name, df in raw.items():
        if isinstance(df, pd.DataFrame) and len(df.columns)>0 and len(df.dropna(how="all"))>0:
            return df
    first = list(raw.values())[0]
    return first if isinstance(first, pd.DataFrame) else pd.DataFrame()

def read_csv_any(fp: str) -> pd.DataFrame:
    for enc in ("utf-8-sig","utf-8","gbk","gb18030"):
        try:
            return pd.read_csv(fp, encoding=enc, dtype=str, low_memory=False)
        except Exception:
            pass
    return pd.read_csv(fp, dtype=str, low_memory=False)

def to_date_str(x, fmt_out="%Y-%m-%d"):
    if pd.isna(x):
        return None
    s = str(x).strip()
    if not s:
        return None
    for fmt in ("%Y-%m-%d","%Y/%m/%d","%Y.%m.%d",
                "%Y-%m-%d %H:%M:%S","%Y/%m/%d %H:%M:%S",
                "%Y-%m-%d %H:%M","%Y/%m/%d %H:%M"):
        try:
            from datetime import datetime
            dt = datetime.strptime(s, fmt)
            return dt.strftime(fmt_out)
        except Exception:
            continue
    try:
        dt = pd.to_datetime(s, errors="coerce")
        if pd.isna(dt):
            return None
        return dt.strftime(fmt_out)
    except Exception:
        return None

import re as _re
def to_float(x):
    if x is None or (isinstance(x, float) and np.isnan(x)):
        return 0.0
    s = str(x).strip().replace(",", "")
    if s == "":
        return 0.0
    if s.endswith("%"):
        try:
            return float(s.strip("%"))/100.0
        except Exception:
            return 0.0
    try:
        return float(s)
    except Exception:
        try:
            return float(_re.sub(r"[^0-9\.\-]", "", s) or 0)
        except Exception:
            return 0.0

def non_empty(val):
    s = str(val).strip().lower()
    return s not in ("", "nan", "none", "-", "null", "æ— ")

def normalize_team(x):
    if pd.isna(x): return ""
    s = str(x).strip()
    if not s: return ""
    names = [n.strip() for n in s.split(",") if n.strip()!=""]
    if not names: return ""
    names = sorted(names, key=lambda z: z)
    return ", ".join(names)

def _winsor_minmax_norm(s: pd.Series, p_low=5, p_high=95):
    s = pd.to_numeric(s, errors="coerce").fillna(0.0)
    if len(s)==0:
        return pd.Series([], dtype=float)
    lo = float(np.nanpercentile(s, p_low))
    hi = float(np.nanpercentile(s, p_high))
    if hi < lo:
        lo, hi = hi, lo
    s_clip = s.clip(lower=lo, upper=hi)
    denom = (hi - lo)
    if denom == 0 or np.isclose(denom, 0):
        return pd.Series(np.zeros(len(s)), index=s.index, dtype=float)
    return (s_clip - lo) / denom

def compute_expo_threshold(expo_series: pd.Series) -> float:
    s = pd.to_numeric(expo_series, errors="coerce").fillna(0.0)
    s = s[s>0]
    if len(s)==0:
        return 5000.0
    p90 = float(np.nanpercentile(s, 90))
    return max(5000.0, p90)

def compute_percent_score(df: pd.DataFrame, expo_col="æ•´ä½“å±•ç°æ¬¡æ•°", expo_threshold: float = 10000.0):
    weights = {
        "roi": 0.35,
        "æ¶ˆè€—é‡‘é¢": 0.25,
        "å¹³å‡ç‚¹å‡»ç‡": 0.15,
        "å¹³å‡è½¬åŒ–ç‡": 0.15,
        "å¹³å‡ 3s å®Œæ’­ç‡": 0.05,
        "æˆäº¤é‡‘é¢": 0.05,
    }
    out = df.copy()
    out["æ¶ˆè€—é‡‘é¢"] = pd.to_numeric(out.get("æ¶ˆè€—é‡‘é¢", 0.0), errors="coerce").fillna(0.0).clip(lower=0.0)
    out["æˆäº¤é‡‘é¢"] = pd.to_numeric(out.get("æˆäº¤é‡‘é¢", 0.0), errors="coerce").fillna(0.0).clip(lower=0.0)
    out["roi"] = out.apply(lambda r: (r["æˆäº¤é‡‘é¢"]/r["æ¶ˆè€—é‡‘é¢"]) if r["æ¶ˆè€—é‡‘é¢"]>0 else 0.0, axis=1)

    n_roi   = _winsor_minmax_norm(out["roi"])
    n_spend = _winsor_minmax_norm(np.log1p(out["æ¶ˆè€—é‡‘é¢"]))
    n_ctr   = _winsor_minmax_norm(pd.to_numeric(out.get("å¹³å‡ç‚¹å‡»ç‡", 0.0), errors="coerce"))
    n_cvr   = _winsor_minmax_norm(pd.to_numeric(out.get("å¹³å‡è½¬åŒ–ç‡", 0.0), errors="coerce"))
    n_v3s   = _winsor_minmax_norm(pd.to_numeric(out.get("å¹³å‡ 3s å®Œæ’­ç‡", 0.0), errors="coerce"))
    n_gmv   = _winsor_minmax_norm(out["æˆäº¤é‡‘é¢"])

    score01 = (
        weights["roi"]            * n_roi   +
        weights["æ¶ˆè€—é‡‘é¢"]        * n_spend +
        weights["å¹³å‡ç‚¹å‡»ç‡"]      * n_ctr   +
        weights["å¹³å‡è½¬åŒ–ç‡"]      * n_cvr   +
        weights["å¹³å‡ 3s å®Œæ’­ç‡"]  * n_v3s   +
        weights["æˆäº¤é‡‘é¢"]        * n_gmv
    )

    expo = pd.to_numeric(out.get(expo_col, 0.0), errors="coerce").fillna(0.0).clip(lower=0.0)
    expo_threshold = float(expo_threshold) if expo_threshold and expo_threshold>0 else 10000.0
    reliability = np.sqrt(expo / expo_threshold).clip(0.0, 1.0)

    out["è¯„åˆ†"] = np.round(100.0 * score01 * reliability, 1)
    return out

def load_register() -> pd.DataFrame:
    files = [f for f in ["åƒå·ç´ æIDç™»è®°è¡¨.xlsx"] if os.path.exists(f)]
    if not files:
        raise FileNotFoundError("æœªæ‰¾åˆ°ã€åƒå·ç´ æIDç™»è®°è¡¨.xlsxã€")
    df = read_excel_any(files[0])
    df.columns = [str(c).strip() for c in df.columns]

    cols = df.columns.tolist()
    id_col = next((c for c in cols if "åƒå·ç´ æID" in c), None)
    col_bd = next((c for c in cols if "ç¼–å¯¼" in c), None)
    col_ps = next((c for c in cols if "æ‹æ‘„" in c), None)
    col_jj = next((c for c in cols if "å‰ªè¾‘" in c), None)
    if not id_col:
        id_col = cols[0]

    reg = pd.DataFrame({
        "åƒå·ç´ æID": df[id_col].astype(str).str.strip(),
        "ç¼–å¯¼": df[col_bd] if col_bd in df.columns else "",
        "æ‹æ‘„": df[col_ps] if col_ps in df.columns else "",
        "å‰ªè¾‘": df[col_jj] if col_jj in df.columns else "",
    })
    reg["æ‹æ‘„"] = reg["æ‹æ‘„"].map(normalize_team)
    reg = reg[ reg["åƒå·ç´ æID"].apply(non_empty) ].drop_duplicates(subset=["åƒå·ç´ æID"])
    return reg

def explode_mapping(reg: pd.DataFrame) -> pd.DataFrame:
    rows = []
    for _, r in reg.iterrows():
        mid = str(r["åƒå·ç´ æID"]).strip()
        for role_col, role_name in [("ç¼–å¯¼","ç¼–å¯¼"),("æ‹æ‘„","æ‹æ‘„"),("å‰ªè¾‘","å‰ªè¾‘")]:
            val = r.get(role_col, "")
            if pd.isna(val) or str(val).strip()=="":
                continue
            names = [n.strip() for n in str(val).split(",") if n.strip()!=""]
            for nm in names:
                rows.append({"åƒå·ç´ æID": mid, "å§“å": nm, "å²—ä½": role_name})
    if not rows:
        return pd.DataFrame(columns=["åƒå·ç´ æID","å§“å","å²—ä½"])
    m = pd.DataFrame(rows).drop_duplicates()
    return m

def _find_col_gmv(cols):
    for c in cols:
        s = str(c)
        if ("æˆäº¤é‡‘é¢" in s) and ("ç‡" not in s):
            return c
    for key in ["æ•´ä½“æˆäº¤é‡‘é¢","GMV","æˆäº¤é‡‘é¢ï¼ˆå…ƒï¼‰","æˆäº¤é‡‘é¢(å…ƒ)","æˆäº¤é‡‘é¢"]:
        for c in cols:
            if key in str(c):
                return c
    return None

def load_material_data() -> pd.DataFrame:
    files = sorted(glob.glob("å…¨åŸŸæ¨å¹¿æ•°æ®-æŠ•åæ•°æ®-ç´ æ-*.xlsx")+glob.glob("å…¨åŸŸæ¨å¹¿æ•°æ®-æŠ•åæ•°æ®-ç´ æ-*.csv"))
    if not files:
        raise FileNotFoundError("æœªæ‰¾åˆ°ã€å…¨åŸŸæ¨å¹¿æ•°æ®-æŠ•åæ•°æ®-ç´ æ-*.xlsx/.csvã€æ–‡ä»¶")
    rows = []
    for fp in files:
        try:
            if fp.lower().endswith(".csv"):
                df = read_csv_any(fp)
            else:
                try:
                    df = read_excel_df(fp, sheet_name="Sheet1")
                except Exception:
                    df = read_excel_any(fp)
            cols = [str(c).strip() for c in df.columns]
            df.columns = cols

            col_date = next((c for c in cols if "æ—¥æœŸ" in c), None)
            col_id   = next((c for c in cols if "ç´ æID" in c or ("ç´ æ" in c and "ID" in c)), None)
            col_cost = next((c for c in cols if "æ•´ä½“æ¶ˆè€—" in c or c=="æ¶ˆè€—"), None)
            col_gmv  = _find_col_gmv(cols)
            col_ctim = next((c for c in cols if "ç´ æåˆ›å»ºæ—¶é—´" in c or "åˆ›å»º" in c), None)
            col_v3s  = next((c for c in cols if "3ç§’æ’­æ”¾ç‡" in c), None)
            col_ctr  = next((c for c in cols if "æ•´ä½“ç‚¹å‡»ç‡" in c or "ç‚¹å‡»ç‡" in c), None)
            col_cvr  = next((c for c in cols if "æ•´ä½“è½¬åŒ–ç‡" in c or "è½¬åŒ–ç‡" in c), None)
            col_expo = next((c for c in cols if "å±•ç°" in c), None)

            need = [col_date,col_id,col_cost,col_ctim,col_v3s,col_ctr,col_cvr]
            if any(c is None for c in need):
                missing = [n for n in need if n is None]
                raise ValueError(f"å¿…éœ€åˆ—ç¼ºå¤±ï¼š{missing} in {fp}")

            gmv_source = None
            if col_gmv is None:
                if df.shape[1] >= 15:
                    col_gmv = df.columns[14]
                    gmv_source = f"(fallback Oåˆ—: {col_gmv})"
                else:
                    raise ValueError(f"æœªæ‰¾åˆ°æˆäº¤é‡‘é¢åˆ—ï¼Œä¸”æ— æ³•æŒ‰Oåˆ—å›é€€ï¼ˆåˆ—æ•°={df.shape[1]}ï¼‰ in {fp}")
            else:
                gmv_source = f"(header: {col_gmv})"

            if col_expo is None:
                df["__expo_zero__"] = 0
                col_expo = "__expo_zero__"

            t = pd.DataFrame({
                "æ—¥æœŸ": df[col_date].map(lambda x: to_date_str(x)),
                "åƒå·ç´ æID": df[col_id].astype(str).str.strip(),
                "æ•´ä½“æ¶ˆè€—": df[col_cost].map(to_float),
                "æ•´ä½“æˆäº¤é‡‘é¢": df[col_gmv].map(to_float),
                "ç´ æåˆ›å»ºæ—¥æœŸ": df[col_ctim].map(lambda x: to_date_str(x)),
                "3ç§’æ’­æ”¾ç‡": df[col_v3s].map(to_float),
                "æ•´ä½“ç‚¹å‡»ç‡": df[col_ctr].map(to_float),
                "æ•´ä½“è½¬åŒ–ç‡": df[col_cvr].map(to_float),
                "æ•´ä½“å±•ç°æ¬¡æ•°": pd.to_numeric(df[col_expo], errors="coerce").fillna(0.0),
            })
            raw_sum = float(pd.to_numeric(df[col_gmv].map(to_float), errors="coerce").fillna(0.0).sum())
            use_sum = float(t["æ•´ä½“æˆäº¤é‡‘é¢"].sum())
            info(f"è¯»å–ï¼š{os.path.basename(fp)} è¡Œ={len(t)}  æˆäº¤åˆ—={gmv_source}  æ–‡ä»¶å†…æ€»æˆäº¤={raw_sum:.2f}  è¯»å–åæ€»æˆäº¤={use_sum:.2f}")

            rows.append(t)
        except Exception as e:
            info(f"âš ï¸ è¯»å–å¤±è´¥ï¼š{fp}  {e}")
    if not rows:
        raise RuntimeError("ç´ ææ˜ç»†è¯»å–ä¸ºç©º")
    mat = pd.concat(rows, ignore_index=True)
    mat = mat[ mat["æ—¥æœŸ"].apply(non_empty) & mat["åƒå·ç´ æID"].apply(non_empty) ]
    return mat

def build_daily_person_role(mat: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:
    daily_mid = (mat.groupby(["æ—¥æœŸ","åƒå·ç´ æID"], as_index=False).agg({
        "æ•´ä½“æ¶ˆè€—":"sum",
        "æ•´ä½“æˆäº¤é‡‘é¢":"sum",
        "æ•´ä½“å±•ç°æ¬¡æ•°":"sum",
        "3ç§’æ’­æ”¾ç‡":"mean",
        "æ•´ä½“ç‚¹å‡»ç‡":"mean",
        "æ•´ä½“è½¬åŒ–ç‡":"mean",
        "ç´ æåˆ›å»ºæ—¥æœŸ":"first"
    }))
    df = daily_mid.merge(mapping, on="åƒå·ç´ æID", how="left")
    df = df[ df["å§“å"].apply(non_empty) & df["å²—ä½"].apply(non_empty) ]

    new_upload = (df.loc[df["ç´ æåˆ›å»ºæ—¥æœŸ"]==df["æ—¥æœŸ"], ["æ—¥æœŸ","å§“å","å²—ä½","åƒå·ç´ æID"]]
                    .drop_duplicates()
                    .groupby(["æ—¥æœŸ","å§“å","å²—ä½"], as_index=False)["åƒå·ç´ æID"].count()
                    .rename(columns={"åƒå·ç´ æID":"æ–°ä¸Šä¼ ä½œå“æ•°"}))

    thresholds = [0,1000,10000,30000,50000,100000]
    th_rows = []
    for _, r in df.iterrows():
        base = {"æ—¥æœŸ": r["æ—¥æœŸ"], "å§“å": r["å§“å"], "å²—ä½": r["å²—ä½"], "åƒå·ç´ æID": r["åƒå·ç´ æID"]}
        for t in thresholds:
            base[f"æ¶ˆè€—ï¼{t} ä½œå“æ•°"] = 1 if r["æ•´ä½“æ¶ˆè€—"]>t else 0
        th_rows.append(base)
    th_df = pd.DataFrame(th_rows).drop_duplicates(subset=["æ—¥æœŸ","å§“å","å²—ä½","åƒå·ç´ æID"])
    th_df = th_df.groupby(["æ—¥æœŸ","å§“å","å²—ä½"], as_index=False).sum()

    sums = (df.groupby(["æ—¥æœŸ","å§“å","å²—ä½"], as_index=False)
              .agg({"æ•´ä½“æ¶ˆè€—":"sum","æ•´ä½“æˆäº¤é‡‘é¢":"sum","æ•´ä½“å±•ç°æ¬¡æ•°":"sum"})
              .rename(columns={"æ•´ä½“æ¶ˆè€—":"æ¶ˆè€—é‡‘é¢","æ•´ä½“æˆäº¤é‡‘é¢":"æˆäº¤é‡‘é¢"}))

    rates = df[df["æ•´ä½“æ¶ˆè€—"]>0].groupby(["æ—¥æœŸ","å§“å","å²—ä½"], as_index=False).agg({
        "3ç§’æ’­æ”¾ç‡":"mean",
        "æ•´ä½“ç‚¹å‡»ç‡":"mean",
        "æ•´ä½“è½¬åŒ–ç‡":"mean"
    }).rename(columns={"3ç§’æ’­æ”¾ç‡":"å¹³å‡ 3s å®Œæ’­ç‡","æ•´ä½“ç‚¹å‡»ç‡":"å¹³å‡ç‚¹å‡»ç‡","æ•´ä½“è½¬åŒ–ç‡":"å¹³å‡è½¬åŒ–ç‡"})

    out = sums.merge(new_upload, on=["æ—¥æœŸ","å§“å","å²—ä½"], how="left") \
              .merge(th_df, on=["æ—¥æœŸ","å§“å","å²—ä½"], how="left") \
              .merge(rates, on=["æ—¥æœŸ","å§“å","å²—ä½"], how="left")

    for c in ["æ–°ä¸Šä¼ ä½œå“æ•°"]+[f"æ¶ˆè€—ï¼{t} ä½œå“æ•°" for t in thresholds]:
        if c in out.columns: out[c] = out[c].fillna(0).astype(int)
    for c in ["å¹³å‡ 3s å®Œæ’­ç‡","å¹³å‡ç‚¹å‡»ç‡","å¹³å‡è½¬åŒ–ç‡"]:
        if c in out.columns: out[c] = out[c].fillna(0.0)
    return out

def build_material_person_role(mat: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:
    daily_mid = (mat.groupby(["æ—¥æœŸ","åƒå·ç´ æID"], as_index=False).agg({
        "æ•´ä½“æ¶ˆè€—":"sum",
        "æ•´ä½“æˆäº¤é‡‘é¢":"sum",
        "æ•´ä½“å±•ç°æ¬¡æ•°":"sum",
        "3ç§’æ’­æ”¾ç‡":"mean",
        "æ•´ä½“ç‚¹å‡»ç‡":"mean",
        "æ•´ä½“è½¬åŒ–ç‡":"mean",
        "ç´ æåˆ›å»ºæ—¥æœŸ":"first"
    }))
    df = daily_mid.merge(mapping, on="åƒå·ç´ æID", how="left")
    df = df[ df["å§“å"].apply(non_empty) & df["å²—ä½"].apply(non_empty) ]

    sums = (df.groupby(["åƒå·ç´ æID","å§“å","å²—ä½"], as_index=False)
              .agg({"æ•´ä½“æ¶ˆè€—":"sum","æ•´ä½“æˆäº¤é‡‘é¢":"sum","æ•´ä½“å±•ç°æ¬¡æ•°":"sum"})
              .rename(columns={"æ•´ä½“æ¶ˆè€—":"æ¶ˆè€—é‡‘é¢","æ•´ä½“æˆäº¤é‡‘é¢":"æˆäº¤é‡‘é¢"}))

    new_upl = (
        df.loc[df["ç´ æåˆ›å»ºæ—¥æœŸ"] == df["æ—¥æœŸ"], ["åƒå·ç´ æID", "å§“å", "å²—ä½"]]
          .drop_duplicates()
          .groupby(["åƒå·ç´ æID", "å§“å", "å²—ä½"], as_index=False)
          .size()
          .rename(columns={"size": "æ–°ä¸Šä¼ ä½œå“æ•°"})
    )

    thresholds = [0,1000,10000,30000,50000,100000]
    th_rows = []
    for _, r in df.iterrows():
        base = {"åƒå·ç´ æID": r["åƒå·ç´ æID"], "å§“å": r["å§“å"], "å²—ä½": r["å²—ä½"], "æ—¥æœŸ": r["æ—¥æœŸ"], "æ•´æ—¥æ¶ˆè€—": r["æ•´ä½“æ¶ˆè€—"]}
        for t in thresholds:
            base[f"æ¶ˆè€—ï¼{t} ä½œå“æ•°"] = 1 if r["æ•´ä½“æ¶ˆè€—"]>t else 0
        th_rows.append(base)
    th_df = pd.DataFrame(th_rows).drop_duplicates(subset=["åƒå·ç´ æID","å§“å","å²—ä½","æ—¥æœŸ"])
    th_sum = th_df.groupby(["åƒå·ç´ æID","å§“å","å²—ä½"], as_index=False).sum().drop(columns=["æ—¥æœŸ","æ•´æ—¥æ¶ˆè€—"], errors="ignore")

    rates = df[df["æ•´ä½“æ¶ˆè€—"]>0].groupby(["åƒå·ç´ æID","å§“å","å²—ä½"], as_index=False).agg({
        "3ç§’æ’­æ”¾ç‡":"mean",
        "æ•´ä½“ç‚¹å‡»ç‡":"mean",
        "æ•´ä½“è½¬åŒ–ç‡":"mean"
    }).rename(columns={"3ç§’æ’­æ”¾ç‡":"å¹³å‡ 3s å®Œæ’­ç‡","æ•´ä½“ç‚¹å‡»ç‡":"å¹³å‡ç‚¹å‡»ç‡","æ•´ä½“è½¬åŒ–ç‡":"å¹³å‡è½¬åŒ–ç‡"})

    out = sums.merge(new_upl, on=["åƒå·ç´ æID","å§“å","å²—ä½"], how="left") \
              .merge(th_sum, on=["åƒå·ç´ æID","å§“å","å²—ä½"], how="left") \
              .merge(rates, on=["åƒå·ç´ æID","å§“å","å²—ä½"], how="left")

    for c in ["æ–°ä¸Šä¼ ä½œå“æ•°"]+[f"æ¶ˆè€—ï¼{t} ä½œå“æ•°" for t in thresholds]:
        if c in out.columns: out[c] = out[c].fillna(0).astype(int)
    for c in ["å¹³å‡ 3s å®Œæ’­ç‡","å¹³å‡ç‚¹å‡»ç‡","å¹³å‡è½¬åŒ–ç‡"]:
        if c in out.columns: out[c] = out[c].fillna(0.0)
    return out

def main():
    try:
        info("â•â• æ‰«æä¸è¯»å– â”€â”€")
        reg = load_register()
        info(f"ç™»è®°è¡¨ï¼š{len(reg)} è¡Œ")

        mapping = explode_mapping(reg)
        info(f"æ˜ å°„è¡Œï¼š{len(mapping)} è¡Œï¼ˆç´ æID-å§“å-å²—ä½ï¼‰")

        mat = load_material_data()
        info(f"ç´ ææ˜ç»†ï¼š{len(mat)} è¡Œ")

        all_dates = pd.to_datetime(mat["æ—¥æœŸ"], errors="coerce").dropna()
        date_span = f"{all_dates.min():%m%d}-{all_dates.max():%m%d}" if len(all_dates)>0 else ""

        expo_threshold = compute_expo_threshold(mat["æ•´ä½“å±•ç°æ¬¡æ•°"])
        info(f"è‡ªåŠ¨æ›å…‰é—¨æ§›ï¼ˆé˜ˆå€¼ï¼‰ï¼š{expo_threshold:.0f}")

        info("â•â• æ„å»º Sheet1ï¼ˆæ—¥æ±‡æ€»ï¼‰ â”€â”€")
        sheet1 = build_daily_person_role(mat, mapping)
        sheet1 = compute_percent_score(sheet1, expo_col="æ•´ä½“å±•ç°æ¬¡æ•°", expo_threshold=expo_threshold)
        ordered1 = ["æ—¥æœŸ","å§“å","å²—ä½","æ¶ˆè€—é‡‘é¢","æˆäº¤é‡‘é¢","roi","æ–°ä¸Šä¼ ä½œå“æ•°",
                    "æ¶ˆè€—ï¼0 ä½œå“æ•°","æ¶ˆè€—ï¼1000 ä½œå“æ•°","æ¶ˆè€—ï¼10000 ä½œå“æ•°",
                    "æ¶ˆè€—ï¼30000 ä½œå“æ•°","æ¶ˆè€—ï¼50000 ä½œå“æ•°","æ¶ˆè€—ï¼100000 ä½œå“æ•°",
                    "æ•´ä½“å±•ç°æ¬¡æ•°","å¹³å‡ 3s å®Œæ’­ç‡","å¹³å‡ç‚¹å‡»ç‡","å¹³å‡è½¬åŒ–ç‡","è¯„åˆ†"]
        for c in ordered1:
            if c not in sheet1.columns: sheet1[c] = 0
        sheet1 = sheet1[ordered1].sort_values(["æ—¥æœŸ","å§“å","å²—ä½"]).reset_index(drop=True)

        info("â•â• æ„å»º Sheet2ï¼ˆç´ æè¯„åˆ†æ˜ç»†ï¼‰ â”€â”€")
        sheet2 = build_material_person_role(mat, mapping)
        sheet2 = compute_percent_score(sheet2, expo_col="æ•´ä½“å±•ç°æ¬¡æ•°", expo_threshold=expo_threshold)
        sheet2.insert(0, "æ±‡æ€»æ—¥æœŸ", date_span)
        ordered2 = ["æ±‡æ€»æ—¥æœŸ","åƒå·ç´ æID","å§“å","å²—ä½","æ¶ˆè€—é‡‘é¢","æˆäº¤é‡‘é¢","roi","æ–°ä¸Šä¼ ä½œå“æ•°",
                    "æ¶ˆè€—ï¼0 ä½œå“æ•°","æ¶ˆè€—ï¼1000 ä½œå“æ•°","æ¶ˆè€—ï¼10000 ä½œå“æ•°",
                    "æ¶ˆè€—ï¼30000 ä½œå“æ•°","æ¶ˆè€—ï¼50000 ä½œå“æ•°","æ¶ˆè€—ï¼100000 ä½œå“æ•°",
                    "æ•´ä½“å±•ç°æ¬¡æ•°","å¹³å‡ 3s å®Œæ’­ç‡","å¹³å‡ç‚¹å‡»ç‡","å¹³å‡è½¬åŒ–ç‡","è¯„åˆ†"]
        for c in ordered2:
            if c not in sheet2.columns: sheet2[c] = 0
        sheet2 = sheet2[ordered2].sort_values(["è¯„åˆ†","æ¶ˆè€—é‡‘é¢","æˆäº¤é‡‘é¢"], ascending=[False, False, False]).reset_index(drop=True)

        info("â•â• æ„å»º Sheet3ï¼ˆç™»è®°è¡¨+è¯„åˆ†ï¼‰ â”€â”€")
        avg_score = sheet2.groupby("åƒå·ç´ æID", as_index=False)["è¯„åˆ†"].mean()
        sheet3 = reg.merge(avg_score, on="åƒå·ç´ æID", how="left")
        sheet3["è¯„åˆ†"] = sheet3["è¯„åˆ†"].fillna(0.0)
        # âœ… ä½¿ç”¨ list é€‰æ‹©åˆ—ï¼Œä¸”ç¼ºåˆ—å…œåº•
        final_cols = ["åƒå·ç´ æID","ç¼–å¯¼","æ‹æ‘„","å‰ªè¾‘","è¯„åˆ†"]
        for c in final_cols:
            if c not in sheet3.columns:
                sheet3[c] = "" if c != "è¯„åˆ†" else 0.0
        sheet3 = sheet3[final_cols].sort_values(["è¯„åˆ†"], ascending=False).reset_index(drop=True)

        info("â•â• å¯¼å‡º Excel â”€â”€")
        with pd.ExcelWriter(OUT_XLSX, engine="openpyxl") as w:
            sheet1.to_excel(w, sheet_name="Sheet1_æ—¥æ±‡æ€»", index=False)
            sheet2.to_excel(w, sheet_name="Sheet2_ç´ æè¯„åˆ†æ˜ç»†", index=False)
            sheet3.to_excel(w, sheet_name="Sheet3_åƒå·ç´ æIDç™»è®°è¡¨", index=False)
        info(f"âœ… å·²è¾“å‡ºï¼š{OUT_XLSX}")
        info(f"ğŸ“„ è¿è¡Œæ—¥å¿—ï¼š{LOG_FILE}")
    except Exception as e:
        info("âŒ å‘ç”Ÿå¼‚å¸¸ï¼š")
        traceback.print_exc(file=sys.stdout)
        info(f"ğŸ“„ è¿è¡Œæ—¥å¿—ï¼š{LOG_FILE}")
        raise

if __name__ == "__main__":
    main()
